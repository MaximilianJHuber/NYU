{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3\n",
    "## Jonathan Elliott\n",
    "## December 8, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanelliott/anaconda/lib/python3.6/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['pi', 'beta']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Computing the HZ Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Underflow safe function\n",
    "def logsumexp(X): # x is a 3-d matrix, len(x_grid) x len(lim_seq) x len([0, 1])\n",
    "    A = np.amax(X, axis=2)\n",
    "    return np.log(np.sum(np.exp(X - A[:,:,None]), axis=2)) + A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate (analytically) the gradient of the log-likelihood function in Rust with respect to the parameters of\n",
    "the model and write down the analytic results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Estimation via MLE and MPEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the model using the NPMLE approach of Rust. You will want to use the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanelliott/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1842.55549794\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "heads = [\"ID\", \"x_t\", \"d_t\"]\n",
    "dataset = pd.read_csv('rustdata1.csv', names=heads)\n",
    "data = {}\n",
    "for head in heads:\n",
    "    data[head.lower()] = np.asfarray(np.array(dataset[head][1:]), float)\n",
    "    \n",
    "# Parameters\n",
    "k = 5 # number of transition probabilities\n",
    "beta = 0.95 # β isn't identified, so need to provide it\n",
    "\n",
    "# Compute transition probabilities from data\n",
    "Delta_xt = np.array([])\n",
    "for i in np.arange(1, len(data['x_t'])):\n",
    "    if data['d_t'][i-1] == 0:\n",
    "        Delta_xt = np.append(Delta_xt, data['x_t'][i] - data['x_t'][i-1])\n",
    "    if data['d_t'][i-1] == 1:\n",
    "        Delta_xt = np.append(Delta_xt, data['x_t'][i] - 0)\n",
    "n = len(Delta_xt)\n",
    "transition_pr = np.array([])\n",
    "lim_seq = np.linspace(0, 2, k)\n",
    "for i in np.arange(1, len(lim_seq)): # I think data in in 1000s\n",
    "    p_hat = np.count_nonzero((Delta_xt >= lim_seq[i-1]) * (Delta_xt < lim_seq[i])) / n\n",
    "    transition_pr = np.append(transition_pr, p_hat)\n",
    "transition_pr = np.append(transition_pr, 1 - np.sum(transition_pr))\n",
    "\n",
    "# Define our arbitrary cost function\n",
    "q = 1 # number of parameters in the following function\n",
    "def c(x, theta_1): # currently defined as a linear function, which apparently Rust preferred\n",
    "    return theta_1 * x\n",
    "\n",
    "# Define profit function\n",
    "def pi(x, i, theta_1, RC): # x is an array, i is not\n",
    "    if i == 0:\n",
    "        pi = -c(x, theta_1)\n",
    "    if i == 1:\n",
    "        pi = -(RC - c(np.zeros(x.shape), theta_1))\n",
    "    return pi\n",
    "\n",
    "# Compute EV(x, θ) via the fixed point\n",
    "l = 4 # how fine the discretized x grid is, l=1 is as fine as Δy from lim_seq, l=2 is twice as fine, etc.\n",
    "def EV_fp(theta_1, RC):\n",
    "    x_max = 14 # maximum value of x - in data, nothing above 13.8...\n",
    "    Delta_y = lim_seq[1] # Δy - the size of the increments in lim_seq\n",
    "    x = np.linspace(0, x_max, x_max/Delta_y*l+1) # need such that it includes the addition of Δy, which is characterized by lim_seq\n",
    "    maxx = np.amax(x) # maximum value in the discretized x grid\n",
    "    i = np.array([0, 1])\n",
    "    \n",
    "    # Construct the payoffs\n",
    "    Deltay = np.tile(lim_seq, (len(x), 1))\n",
    "    xplusDeltay = np.tile(x, (len(lim_seq), 1)).T + Deltay # matrix where each column is x + multiple of Δy\n",
    "    xplusDeltay = np.where(xplusDeltay <= maxx, xplusDeltay, maxx) # keep the highest values of x within the specified domain\n",
    "    idx = np.tile(np.arange(len(x)), (len(lim_seq), 1)).T + np.arange(len(lim_seq)) * l\n",
    "    idx = np.where(idx <= len(x) - 1, idx, len(x) - 1) # index used to differentially move up columns\n",
    "    pi0 = pi(xplusDeltay, 0, theta_1, RC)\n",
    "    pi1 = pi(xplusDeltay, 1, theta_1, RC)\n",
    "    \n",
    "    # Construct p(x_t+1 | i=1)\n",
    "    pr_i1 = np.vstack((np.ones(len(lim_seq)), np.zeros((len(x) - 1, len(lim_seq)))))\n",
    "    \n",
    "    # Initialize the loop\n",
    "    init_EV = np.zeros((len(x), len(i)))\n",
    "    EV_tau = init_EV\n",
    "    err = 1\n",
    "    err_tol = 1e-12\n",
    "    iter_num = 1\n",
    "    iter_lim = 5000\n",
    "    while err > err_tol and iter_num < iter_lim:\n",
    "        pdv0 = pi0 + beta * (np.tile(EV_tau[:, 0], (len(lim_seq), 1)).T)[idx, np.arange(len(lim_seq))]\n",
    "        pdv1 = pi1 + beta * (np.tile(EV_tau[:, 1], (len(lim_seq), 1)).T)[idx, np.arange(len(lim_seq))]\n",
    "        logsumexp_prepr = logsumexp(np.dstack((pdv0, pdv1)))\n",
    "        EV_tau1_i0 = np.sum(logsumexp_prepr * transition_pr, axis=1)\n",
    "        EV_tau1_i1 = np.sum(logsumexp_prepr * pr_i1, axis=1)\n",
    "        EV_tau1 = np.vstack((EV_tau1_i0, EV_tau1_i1)).T\n",
    "        err = np.amax(np.abs(EV_tau1 - EV_tau))\n",
    "        EV_tau = EV_tau1\n",
    "        iter_num += 1\n",
    "    if iter_num == iter_lim:\n",
    "        print(\"EV didn't converge.\")\n",
    "    return x, EV_tau\n",
    "\n",
    "# Define EV function\n",
    "def EV_fct(EV_fp, x_space, x, i): # x is an array, i is not\n",
    "    return np.interp(x, x_space, EV_fp[:,i]) # interpolate \n",
    "\n",
    "# Define choice-specific value function\n",
    "def v(x, i, theta_1, RC, EV, x_space): # x is an array, i is not\n",
    "    v = pi(x, i, theta_1, RC) + beta * EV_fct(EV, x_space, x, i)\n",
    "    return v\n",
    "\n",
    "# Construct CCP given EV(x, θ)\n",
    "def CCP(x, i, theta_1, RC):\n",
    "    x_space, EV = EV_fp(theta_1, RC)\n",
    "    V1 = v(x, 1, theta_1, RC, EV, x_space)\n",
    "    V0 = v(x, 0, theta_1, RC, EV, x_space)\n",
    "    Vi = V0*(i == 0) + V1*(i == 1)\n",
    "    pr = np.divide(np.exp(Vi), np.exp(V0) + np.exp(V1))\n",
    "    return pr\n",
    "\n",
    "# Construct the likelihood\n",
    "def loglikelihood(theta, x, i):\n",
    "    theta_1 = theta[:-1]\n",
    "    RC = theta[-1]\n",
    "    logl = np.sum(np.log(CCP(x, i, theta_1, RC)))\n",
    "    return -logl # negative because we are using a minimizer, but it's MLE\n",
    "\n",
    "# Construct likelihood's gradient\n",
    "def loglikelihood_grad(theta, x, i): # must accept same arguments as likelihood()\n",
    "    return 0 # need to compute this by hand (and write it in question 1)\n",
    "\n",
    "# Solve via MLE\n",
    "init_guess = np.zeros(q + 1)\n",
    "# res = minimize(loglikelihood, init_guess, args=(data['x_t'], data['d_t']), method='BFGS', jac=loglikelihood_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the model using the MPEC method of Su and Judd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.]\n",
      "[[ 1.  1.  1.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(3)\n",
    "print(a)\n",
    "b = np.zeros((5,3))\n",
    "print(np.vstack((a, b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results in a table, including the nonparametric answers below and discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iter_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-52ae826b69a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0miter_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'iter_num' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the $EV(\\cdot)$ you have obtained for both estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: The Stata Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is taken from Han Hong’s problem set at Stanford, the idea is that we can use the arguments in Hotz-Miller (1993), or Pesendorfer Schmidt-Dengler (2008) to construct an optimization free method to recover the utility pa- rameters in the Rust problem.\n",
    "\n",
    "We begin by defining the choice specific value function with $\\varepsilon_{it}$ i.i.d. and EV.\n",
    "\n",
    "$$v(x,d) = u(x,d) + \\beta \\int \\log \\left( \\sum_{d' \\in D} \\exp(v(x', d')) \\right) p(x'|x, d) \\mathrm{d}x'$$\n",
    "$$v(x,d) = u(x,d) + \\beta \\int \\log \\left( \\sum_{d' \\in D} \\exp(v(x', d') - v(x', 1)) \\right) p(x'|x, d) \\mathrm{d}x' + \\beta \\int v(x,1)p(x'|x,1) \\mathrm{d}x'$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Estimate $p(x′|x,d)$ non parametrically or parametrically (for example as a set of multinomial with $n$ outcomes\n",
    "or an exponential distribution). Call your estimate $\\hat{p}(x′|x,d)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Estimate $p(d|x)$ (the CCP) non-parametrically. You can use the binomial logit model with a basis function\n",
    "(increasing number of terms) or you can use a kernel such as **ksdensity** or **ecdf**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Now use the Hotz-Miller inversion to estimate: $\\hat{v}(x, d) − \\hat{v}(x, 1) = \\log \\hat{p}(d|x) − \\log \\hat{p}(1|x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
